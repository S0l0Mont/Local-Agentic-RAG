{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S0l0Mont/Local-Agentic-RAG/blob/main/RAG_Llama_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7Ye4-KpUJc0"
      },
      "source": [
        "# <font color='289C4E'>1. Import Libraries ğŸ“š<font><a class='anchor' id='lib'></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r /content/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "WJUe50clVZLP",
        "outputId": "149a467c-ab19-4d51-8ee7-8ce8c449a988"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 1)) (5.5.6)\n",
            "Collecting openai (from -r /content/requirements.txt (line 2))\n",
            "  Downloading openai-1.35.10-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from -r /content/requirements.txt (line 3))\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere (from -r /content/requirements.txt (line 4))\n",
            "  Downloading cohere-5.5.8-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama_index (from -r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index-0.10.53-py3-none-any.whl (6.8 kB)\n",
            "Collecting pypdf (from -r /content/requirements.txt (line 6))\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers (from -r /content/requirements.txt (line 7))\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 8)) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 9)) (0.18.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 10)) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 11)) (0.18.0)\n",
            "Collecting deeplake[all] (from -r /content/requirements.txt (line 12))\n",
            "  Downloading deeplake-3.9.12.tar.gz (606 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m606.9/606.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r /content/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r /content/requirements.txt (line 1)) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r /content/requirements.txt (line 1)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r /content/requirements.txt (line 1)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r /content/requirements.txt (line 1)) (6.3.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r /content/requirements.txt (line 2)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r /content/requirements.txt (line 2))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/requirements.txt (line 2)) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/requirements.txt (line 2)) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->-r /content/requirements.txt (line 3)) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->-r /content/requirements.txt (line 3)) (2.31.0)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere->-r /content/requirements.txt (line 4))\n",
            "  Downloading boto3-1.34.141-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2.0.0,>=1.9.4 (from cohere->-r /content/requirements.txt (line 4))\n",
            "  Downloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx-sse<0.5.0,>=0.4.0 (from cohere->-r /content/requirements.txt (line 4))\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere->-r /content/requirements.txt (line 4))\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere->-r /content/requirements.txt (line 4)) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere->-r /content/requirements.txt (line 4))\n",
            "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core==0.10.53.post1 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_core-0.10.53.post1-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.2.3-py3-none-any.whl (9.2 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_llms_openai-0.1.25-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl (5.9 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_readers_file-0.1.29-py3-none-any.whl (38 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (3.9.5)\n",
            "Collecting dataclasses-json (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (2023.6.0)\n",
            "Collecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (9.4.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (8.4.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (1.14.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r /content/requirements.txt (line 7)) (4.41.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r /content/requirements.txt (line 7)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r /content/requirements.txt (line 7)) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r /content/requirements.txt (line 7)) (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/requirements.txt (line 8)) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/requirements.txt (line 8)) (1.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/requirements.txt (line 8)) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r /content/requirements.txt (line 8))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/requirements.txt (line 8)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r /content/requirements.txt (line 8))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow>=9.0.0 (from llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake[all]->-r /content/requirements.txt (line 12)) (8.1.7)\n",
            "Collecting pathos (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Collecting lz4 (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake[all]->-r /content/requirements.txt (line 12)) (2.3.0)\n",
            "Collecting libdeeplake==0.0.134 (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading libdeeplake-0.0.134-cp310-cp310-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aioboto3>=10.4.0 (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading aioboto3-13.1.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from deeplake[all]->-r /content/requirements.txt (line 12)) (2.2.5)\n",
            "Collecting azure-cli (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_cli-2.61.0-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-identity (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_identity-1.17.1-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-storage~=1.42.0 (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.0/106.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth~=2.0.1 (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading google_auth-2.0.2-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.3/152.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from deeplake[all]->-r /content/requirements.txt (line 12)) (4.0.2)\n",
            "Collecting laspy (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading laspy-2.5.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m391.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-python-client~=2.31.0 (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading google_api_python_client-2.31.0-py2.py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-oauthlib~=0.4.5 (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting azure-storage-blob (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_storage_blob-12.20.0-py3-none-any.whl (392 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m392.2/392.2 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauth2client~=4.1.3 in /usr/local/lib/python3.10/dist-packages (from deeplake[all]->-r /content/requirements.txt (line 12)) (4.1.3)\n",
            "Collecting pydicom (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading pydicom-2.4.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting av>=8.1.0 (from deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading av-12.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from libdeeplake==0.0.134->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiobotocore[boto3]==2.13.1 (from aioboto3>=10.4.0->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading aiobotocore-2.13.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles>=23.2.1 (from aioboto3>=10.4.0->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Collecting botocore<1.34.132,>=1.34.70 (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading botocore-1.34.131-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere->-r /content/requirements.txt (line 4))\n",
            "  Downloading boto3-1.34.131-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r /content/requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r /content/requirements.txt (line 2)) (1.2.1)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere->-r /content/requirements.txt (line 4))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere->-r /content/requirements.txt (line 4))\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.31.0->deeplake[all]->-r /content/requirements.txt (line 12)) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.31.0->deeplake[all]->-r /content/requirements.txt (line 12)) (0.1.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.31.0->deeplake[all]->-r /content/requirements.txt (line 12)) (2.16.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.31.0->deeplake[all]->-r /content/requirements.txt (line 12)) (4.1.1)\n",
            "Collecting cachetools<5.0,>=2.0.0 (from google-auth~=2.0.1->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0.1->deeplake[all]->-r /content/requirements.txt (line 12)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0.1->deeplake[all]->-r /content/requirements.txt (line 12)) (4.9)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0.1->deeplake[all]->-r /content/requirements.txt (line 12)) (67.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib~=0.4.5->deeplake[all]->-r /content/requirements.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage~=1.42.0->deeplake[all]->-r /content/requirements.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage~=1.42.0->deeplake[all]->-r /content/requirements.txt (line 12)) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage~=1.42.0->deeplake[all]->-r /content/requirements.txt (line 12)) (2.7.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage~=1.42.0->deeplake[all]->-r /content/requirements.txt (line 12)) (3.20.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r /content/requirements.txt (line 2)) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r /content/requirements.txt (line 2))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r /content/requirements.txt (line 2))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r /content/requirements.txt (line 7)) (24.1)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1)) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1)) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index->-r /content/requirements.txt (line 5)) (4.12.3)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading llama_parse-0.4.6-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client~=4.1.3->deeplake[all]->-r /content/requirements.txt (line 12)) (0.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r /content/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r /content/requirements.txt (line 2)) (2.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->-r /content/requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->-r /content/requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->-r /content/requirements.txt (line 7)) (0.4.3)\n",
            "Collecting antlr4-python3-runtime~=4.13.1 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading antlr4_python3_runtime-4.13.1-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-appconfiguration~=1.1.1 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_appconfiguration-1.1.1-py2.py3-none-any.whl (37 kB)\n",
            "Collecting azure-batch~=14.2.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_batch-14.2.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-cli-core==2.61.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_cli_core-2.61.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-cosmos>=3.0.2,~=3.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_cosmos-3.2.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-data-tables==12.4.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_data_tables-12.4.0-py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-datalake-store~=0.0.49 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_datalake_store-0.0.53-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-graphrbac~=0.60.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_graphrbac-0.60.0-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.7/139.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-keyvault-administration==4.4.0b2 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_keyvault_administration-4.4.0b2-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-keyvault-certificates==4.7.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_keyvault_certificates-4.7.0-py3-none-any.whl (428 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m428.1/428.1 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-keyvault-keys==4.9.0b3 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_keyvault_keys-4.9.0b3-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-keyvault-secrets==4.7.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_keyvault_secrets-4.7.0-py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m348.6/348.6 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-advisor==9.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_advisor-9.0.0-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-apimanagement==4.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_apimanagement-4.0.0-py3-none-any.whl (804 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m804.5/804.5 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-appconfiguration==3.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_appconfiguration-3.0.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m252.0/252.0 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-appcontainers==2.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_appcontainers-2.0.0-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m214.1/214.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-applicationinsights~=1.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_applicationinsights-1.0.0-py2.py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-authorization~=4.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_authorization-4.0.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-batchai==7.0.0b1 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_batchai-7.0.0b1-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-batch~=17.3.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_batch-17.3.0-py3-none-any.whl (162 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-billing==6.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_billing-6.0.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.0/167.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-botservice~=2.0.0b3 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_botservice-2.0.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-cdn==12.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_cdn-12.0.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-cognitiveservices~=13.5.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_cognitiveservices-13.5.0-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-compute~=31.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_compute-31.0.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-containerinstance==10.1.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_containerinstance-10.1.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-containerregistry==10.3.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_containerregistry-10.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-containerservice~=30.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_containerservice-30.0.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-cosmosdb==9.4.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_cosmosdb-9.4.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m401.0/401.0 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-databoxedge~=1.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_databoxedge-1.0.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-datalake-store~=0.5.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_datalake_store-0.5.0-py2.py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-datamigration~=10.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_datamigration-10.0.0-py2.py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-devtestlabs~=4.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_devtestlabs-4.0.0-py2.py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-dns~=8.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_dns-8.0.0-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-eventgrid==10.2.0b2 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_eventgrid-10.2.0b2-py3-none-any.whl (248 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m248.5/248.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-eventhub~=10.1.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_eventhub-10.1.0-py3-none-any.whl (598 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m598.9/598.9 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-extendedlocation==1.0.0b2 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_extendedlocation-1.0.0b2-py2.py3-none-any.whl (37 kB)\n",
            "Collecting azure-mgmt-hdinsight~=9.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_hdinsight-9.0.0-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-imagebuilder~=1.3.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_imagebuilder-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-iotcentral~=10.0.0b1 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_iotcentral-10.0.0b2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-iothub==3.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_iothub-3.0.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-iothubprovisioningservices==1.1.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_iothubprovisioningservices-1.1.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-keyvault==10.3.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_keyvault-10.3.0-py3-none-any.whl (933 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m933.0/933.0 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-kusto~=0.3.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_kusto-0.3.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-loganalytics==13.0.0b4 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_loganalytics-13.0.0b4-py3-none-any.whl (162 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-managedservices~=1.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_managedservices-1.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting azure-mgmt-managementgroups~=1.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_managementgroups-1.0.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-maps~=2.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_maps-2.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting azure-mgmt-marketplaceordering==1.1.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_marketplaceordering-1.1.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting azure-mgmt-media~=9.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_media-9.0.0-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-monitor~=5.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_monitor-5.0.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-msi~=7.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_msi-7.0.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-netapp~=10.1.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_netapp-10.1.0-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.7/200.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-policyinsights==1.1.0b4 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_policyinsights-1.1.0b4-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.0/127.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-privatedns~=1.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_privatedns-1.0.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-rdbms~=10.2.0b16 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_rdbms-10.2.0b17-py3-none-any.whl (975 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m975.1/975.1 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-recoveryservicesbackup~=9.1.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_recoveryservicesbackup-9.1.0-py3-none-any.whl (570 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m570.9/570.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-recoveryservices~=3.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_recoveryservices-3.0.0-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-redis~=14.3.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_redis-14.3.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-redhatopenshift~=1.4.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_redhatopenshift-1.4.0-py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.5/345.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-resource==23.1.1 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_resource-23.1.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-search~=9.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_search-9.1.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.3/110.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-security==6.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_security-6.0.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-servicebus~=8.2.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_servicebus-8.2.0-py3-none-any.whl (940 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m940.1/940.1 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-servicefabricmanagedclusters==2.0.0b6 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_servicefabricmanagedclusters-2.0.0b6-py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m204.4/204.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-servicelinker==1.2.0b2 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_servicelinker-1.2.0b2-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-servicefabric~=2.1.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_servicefabric-2.1.0-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-signalr==2.0.0b1 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_signalr-2.0.0b1-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-sqlvirtualmachine==1.0.0b5 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_sqlvirtualmachine-1.0.0b5-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-sql==4.0.0b16 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_sql-4.0.0b16-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-storage==21.1.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_storage-21.1.0-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-synapse==2.1.0b5 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_synapse-2.1.0b5-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.1/547.1 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-trafficmanager~=1.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_trafficmanager-1.0.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-web==7.2.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_web-7.2.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-monitor-query==1.2.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_monitor_query-1.2.0-py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-multiapi-storage~=1.2.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_multiapi_storage-1.2.0-py2.py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-storage-common~=1.4 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-synapse-accesscontrol~=0.5.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_synapse_accesscontrol-0.5.0-py2.py3-none-any.whl (30 kB)\n",
            "Collecting azure-synapse-artifacts~=0.18.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_synapse_artifacts-0.18.0-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m486.7/486.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-synapse-managedprivateendpoints~=0.4.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_synapse_managedprivateendpoints-0.4.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-synapse-spark~=0.2.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_synapse_spark-0.2.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: chardet~=5.2.0 in /usr/local/lib/python3.10/dist-packages (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12)) (5.2.0)\n",
            "Collecting colorama~=0.4.4 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting fabric~=3.2.2 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading fabric-3.2.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting javaproperties~=0.5.1 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading javaproperties-0.5.2-py2.py3-none-any.whl (19 kB)\n",
            "Collecting jsondiff~=2.0.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading jsondiff-2.0.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting pycomposefile>=0.0.29 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading pycomposefile-0.0.31-py3-none-any.whl (28 kB)\n",
            "Collecting PyGithub~=1.38 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading PyGithub-1.59.1-py3-none-any.whl (342 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m342.2/342.2 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyNaCl~=1.5.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scp~=0.13.2 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading scp-0.13.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting semver==2.13.0 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting sshtunnel~=0.1.4 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading sshtunnel-0.1.5-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12)) (0.9.0)\n",
            "Collecting websocket-client~=1.3.1 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xmltodict~=0.12 (from azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting argcomplete~=3.3.0 (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading argcomplete-3.3.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-cli-telemetry==1.1.0.* (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_cli_telemetry-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting azure-mgmt-core<2,>=1.2.0 (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12)) (42.0.8)\n",
            "Collecting humanfriendly~=10.0 (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting knack~=0.11.0 (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading knack-0.11.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msal-extensions==1.2.0b1 (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading msal_extensions-1.2.0b1-py3-none-any.whl (19 kB)\n",
            "Collecting msal[broker]==1.28.0 (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading msal-1.28.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msrestazure~=0.6.4 (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading msrestazure-0.6.4.post1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting paramiko<4.0.0,>=2.0.8 (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading paramiko-3.4.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m225.9/225.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pkginfo>=1.5.0.1 (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading pkginfo-1.11.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: pyopenssl>=17.1.0 in /usr/local/lib/python3.10/dist-packages (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12)) (24.1.0)\n",
            "Requirement already satisfied: psutil~=5.9 in /usr/local/lib/python3.10/dist-packages (from azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12)) (5.9.5)\n",
            "Collecting azure-core<2.0.0,>=1.15.0 (from azure-data-tables==12.4.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_core-1.30.2-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.3/194.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msrest>=0.6.21 (from azure-data-tables==12.4.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-common~=1.1 (from azure-keyvault-administration==4.4.0b2->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-keyvault-administration==4.4.0b2->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting applicationinsights<0.12,>=0.11.1 (from azure-cli-telemetry==1.1.0.*->azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading applicationinsights-0.11.10-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker<3,>=1.6 (from azure-cli-telemetry==1.1.0.*->azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n",
            "Collecting msal<1.29,>=1.27 (from msal-extensions==1.2.0b1->azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading msal-1.28.1-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->deeplake[all]->-r /content/requirements.txt (line 12)) (3.0.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->deeplake[all]->-r /content/requirements.txt (line 12)) (2.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r /content/requirements.txt (line 8)) (2.1.5)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->-r /content/requirements.txt (line 1)) (5.7.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->-r /content/requirements.txt (line 1)) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->-r /content/requirements.txt (line 1)) (2.8.2)\n",
            "Collecting ppft>=1.7.6.8 (from pathos->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.4 (from pathos->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.16 (from pathos->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r /content/requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r /content/requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r /content/requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (4.0.3)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from azure-datalake-store~=0.0.49->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12)) (1.16.0)\n",
            "Collecting azure-mgmt-datalake-nspkg>=2.0.0 (from azure-mgmt-datalake-store~=0.5.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_datalake_nspkg-3.0.1-py3-none-any.whl (1.7 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index->-r /content/requirements.txt (line 5)) (2.5)\n",
            "Collecting invoke>=2.0 (from fabric~=3.2.2->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading invoke-2.2.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decorator (from ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client~=2.31.0->deeplake[all]->-r /content/requirements.txt (line 12)) (1.63.2)\n",
            "INFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-api-core<3.0.0dev,>=1.21.0 (from google-api-python-client~=2.31.0->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client~=2.31.0->deeplake[all]->-r /content/requirements.txt (line 12)) (1.24.0)\n",
            "  Downloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.17.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.17.0-py3-none-any.whl (136 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m136.9/136.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.16.1-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.2/135.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.16.0-py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.4/134.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_api_core-2.15.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.0/122.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.14.0-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.2/122.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.13.1-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.0/122.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.13.0-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.0/122.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.12.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.5/120.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage~=1.42.0->deeplake[all]->-r /content/requirements.txt (line 12)) (1.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client~=2.31.0->deeplake[all]->-r /content/requirements.txt (line 12)) (3.1.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1)) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel->-r /content/requirements.txt (line 1)) (4.2.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->-r /content/requirements.txt (line 1)) (0.2.13)\n",
            "Collecting PyJWT[crypto]<3,>=1.0.0 (from msal[broker]==1.28.0->azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib~=0.4.5->deeplake[all]->-r /content/requirements.txt (line 12)) (3.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5))\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.53.post1->llama_index->-r /content/requirements.txt (line 5)) (2024.1)\n",
            "Collecting azure-mgmt-nspkg>=3.0.0 (from azure-mgmt-datalake-nspkg>=2.0.0->azure-mgmt-datalake-store~=0.5.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_mgmt_nspkg-3.0.2-py3-none-any.whl (1.6 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->azure-datalake-store~=0.0.49->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12)) (2.22)\n",
            "Collecting adal<2.0.0,>=0.6.0 (from msrestazure~=0.6.4->azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bcrypt>=3.2 (from paramiko<4.0.0,>=2.0.8->azure-cli-core==2.61.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->-r /content/requirements.txt (line 3)) (1.7.1)\n",
            "Collecting azure-nspkg>=3.0.0 (from azure-mgmt-nspkg>=3.0.0->azure-mgmt-datalake-nspkg>=2.0.0->azure-mgmt-datalake-store~=0.5.0->azure-cli->deeplake[all]->-r /content/requirements.txt (line 12))\n",
            "  Downloading azure_nspkg-3.0.2-py3-none-any.whl (1.5 kB)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.9.12-py3-none-any.whl size=729493 sha256=8faced70ead1073ed756500f583ab6b6893b3de26b7a50fa363110ecc135bda0\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/f4/28/0d59f27e5508ea19ce3dd781ea9ebbb502965cdf19939383d8\n",
            "Successfully built deeplake\n",
            "Installing collected packages: striprtf, jsondiff, dirtyjson, azure-nspkg, azure-common, applicationinsights, antlr4-python3-runtime, xmltodict, websocket-client, types-requests, semver, pypdf, PyJWT, pydicom, pycomposefile, ppft, pox, portalocker, pkginfo, pillow, parameterized, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, lz4, laspy, jmespath, jedi, javaproperties, isodate, invoke, humanfriendly, httpx-sse, h11, fastavro, dill, deprecated, decorator, colorama, cachetools, bcrypt, azure-mgmt-nspkg, av, argcomplete, aioitertools, aiofiles, typing-inspect, tiktoken, PyNaCl, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, libdeeplake, knack, humbug, httpcore, google-auth, botocore, azure-mgmt-datalake-nspkg, azure-cosmos, azure-core, azure-cli-telemetry, s3transfer, pathos, paramiko, nvidia-cusolver-cu12, msrest, httpx, google-auth-oauthlib, google-api-core, dataclasses-json, azure-storage-common, azure-storage-blob, azure-monitor-query, azure-mgmt-core, azure-keyvault-secrets, azure-keyvault-keys, azure-keyvault-certificates, azure-keyvault-administration, aiobotocore, adal, sshtunnel, scp, PyGithub, openai, msrestazure, msal, llama-cloud, google-api-python-client, fabric, boto3, azure-synapse-spark, azure-synapse-managedprivateendpoints, azure-synapse-artifacts, azure-synapse-accesscontrol, azure-multiapi-storage, azure-mgmt-web, azure-mgmt-trafficmanager, azure-mgmt-synapse, azure-mgmt-storage, azure-mgmt-sqlvirtualmachine, azure-mgmt-sql, azure-mgmt-signalr, azure-mgmt-servicelinker, azure-mgmt-servicefabricmanagedclusters, azure-mgmt-servicefabric, azure-mgmt-servicebus, azure-mgmt-security, azure-mgmt-search, azure-mgmt-resource, azure-mgmt-redis, azure-mgmt-redhatopenshift, azure-mgmt-recoveryservicesbackup, azure-mgmt-recoveryservices, azure-mgmt-rdbms, azure-mgmt-privatedns, azure-mgmt-policyinsights, azure-mgmt-netapp, azure-mgmt-msi, azure-mgmt-monitor, azure-mgmt-media, azure-mgmt-marketplaceordering, azure-mgmt-maps, azure-mgmt-managementgroups, azure-mgmt-loganalytics, azure-mgmt-keyvault, azure-mgmt-iothubprovisioningservices, azure-mgmt-iothub, azure-mgmt-iotcentral, azure-mgmt-imagebuilder, azure-mgmt-hdinsight, azure-mgmt-extendedlocation, azure-mgmt-eventhub, azure-mgmt-eventgrid, azure-mgmt-dns, azure-mgmt-datamigration, azure-mgmt-databoxedge, azure-mgmt-cosmosdb, azure-mgmt-containerservice, azure-mgmt-containerregistry, azure-mgmt-containerinstance, azure-mgmt-compute, azure-mgmt-cognitiveservices, azure-mgmt-cdn, azure-mgmt-botservice, azure-mgmt-billing, azure-mgmt-batchai, azure-mgmt-batch, azure-mgmt-authorization, azure-mgmt-applicationinsights, azure-mgmt-appcontainers, azure-mgmt-appconfiguration, azure-mgmt-apimanagement, azure-mgmt-advisor, azure-data-tables, azure-appconfiguration, sentence-transformers, msal-extensions, llama-index-legacy, llama-index-core, google-cloud-storage, cohere, azure-mgmt-managedservices, azure-mgmt-kusto, azure-mgmt-devtestlabs, azure-mgmt-datalake-store, azure-graphrbac, azure-datalake-store, azure-batch, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, azure-identity, azure-cli-core, aioboto3, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, deeplake, azure-cli, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "  Attempting uninstall: websocket-client\n",
            "    Found existing installation: websocket-client 1.8.0\n",
            "    Uninstalling websocket-client-1.8.0:\n",
            "      Successfully uninstalled websocket-client-1.8.0\n",
            "  Attempting uninstall: PyJWT\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.3\n",
            "    Uninstalling cachetools-5.3.3:\n",
            "      Successfully uninstalled cachetools-5.3.3\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.27.0\n",
            "    Uninstalling google-auth-2.27.0:\n",
            "      Successfully uninstalled google-auth-2.27.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.16.2\n",
            "    Uninstalling google-api-core-2.16.2:\n",
            "      Successfully uninstalled google-api-core-2.16.2\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 2.8.0\n",
            "    Uninstalling google-cloud-storage-2.8.0:\n",
            "      Successfully uninstalled google-cloud-storage-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.11.0 requires google-auth<3.0dev,>=2.15.0, but you have google-auth 2.0.2 which is incompatible.\n",
            "bigframes 1.11.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.42.3 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.4 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.4 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 2.0.2 which is incompatible.\n",
            "google-cloud-aiplatform 1.57.0 requires google-auth<3.0.0dev,>=2.14.1, but you have google-auth 2.0.2 which is incompatible.\n",
            "google-cloud-bigquery 3.21.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-bigquery 3.21.0 requires google-auth<3.0.0dev,>=2.14.1, but you have google-auth 2.0.2 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires google-auth<3.0.0dev,>=2.14.1, but you have google-auth 2.0.2 which is incompatible.\n",
            "google-cloud-bigtable 2.24.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 2.0.2 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 2.0.2 which is incompatible.\n",
            "google-cloud-pubsub 2.21.5 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-pubsub 2.21.5 requires google-auth<3.0.0dev,>=2.14.1, but you have google-auth 2.0.2 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 2.0.2 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.0.2 which is incompatible.\n",
            "google-generativeai 0.5.4 requires google-auth>=2.15.0, but you have google-auth 2.0.2 which is incompatible.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth>=2.13.0, but you have google-auth 2.0.2 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorboard 2.15.2 requires google-auth-oauthlib<2,>=0.5, but you have google-auth-oauthlib 0.4.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyGithub-1.59.1 PyJWT-2.8.0 PyNaCl-1.5.0 adal-1.2.7 aioboto3-13.1.0 aiobotocore-2.13.1 aiofiles-24.1.0 aioitertools-0.11.0 antlr4-python3-runtime-4.13.1 applicationinsights-0.11.10 argcomplete-3.3.0 av-12.2.0 azure-appconfiguration-1.1.1 azure-batch-14.2.0 azure-cli-2.61.0 azure-cli-core-2.61.0 azure-cli-telemetry-1.1.0 azure-common-1.1.28 azure-core-1.30.2 azure-cosmos-3.2.0 azure-data-tables-12.4.0 azure-datalake-store-0.0.53 azure-graphrbac-0.60.0 azure-identity-1.17.1 azure-keyvault-administration-4.4.0b2 azure-keyvault-certificates-4.7.0 azure-keyvault-keys-4.9.0b3 azure-keyvault-secrets-4.7.0 azure-mgmt-advisor-9.0.0 azure-mgmt-apimanagement-4.0.0 azure-mgmt-appconfiguration-3.0.0 azure-mgmt-appcontainers-2.0.0 azure-mgmt-applicationinsights-1.0.0 azure-mgmt-authorization-4.0.0 azure-mgmt-batch-17.3.0 azure-mgmt-batchai-7.0.0b1 azure-mgmt-billing-6.0.0 azure-mgmt-botservice-2.0.0 azure-mgmt-cdn-12.0.0 azure-mgmt-cognitiveservices-13.5.0 azure-mgmt-compute-31.0.0 azure-mgmt-containerinstance-10.1.0 azure-mgmt-containerregistry-10.3.0 azure-mgmt-containerservice-30.0.0 azure-mgmt-core-1.4.0 azure-mgmt-cosmosdb-9.4.0 azure-mgmt-databoxedge-1.0.0 azure-mgmt-datalake-nspkg-3.0.1 azure-mgmt-datalake-store-0.5.0 azure-mgmt-datamigration-10.0.0 azure-mgmt-devtestlabs-4.0.0 azure-mgmt-dns-8.0.0 azure-mgmt-eventgrid-10.2.0b2 azure-mgmt-eventhub-10.1.0 azure-mgmt-extendedlocation-1.0.0b2 azure-mgmt-hdinsight-9.0.0 azure-mgmt-imagebuilder-1.3.0 azure-mgmt-iotcentral-10.0.0b2 azure-mgmt-iothub-3.0.0 azure-mgmt-iothubprovisioningservices-1.1.0 azure-mgmt-keyvault-10.3.0 azure-mgmt-kusto-0.3.0 azure-mgmt-loganalytics-13.0.0b4 azure-mgmt-managedservices-1.0.0 azure-mgmt-managementgroups-1.0.0 azure-mgmt-maps-2.0.0 azure-mgmt-marketplaceordering-1.1.0 azure-mgmt-media-9.0.0 azure-mgmt-monitor-5.0.1 azure-mgmt-msi-7.0.0 azure-mgmt-netapp-10.1.0 azure-mgmt-nspkg-3.0.2 azure-mgmt-policyinsights-1.1.0b4 azure-mgmt-privatedns-1.0.0 azure-mgmt-rdbms-10.2.0b17 azure-mgmt-recoveryservices-3.0.0 azure-mgmt-recoveryservicesbackup-9.1.0 azure-mgmt-redhatopenshift-1.4.0 azure-mgmt-redis-14.3.0 azure-mgmt-resource-23.1.1 azure-mgmt-search-9.1.0 azure-mgmt-security-6.0.0 azure-mgmt-servicebus-8.2.0 azure-mgmt-servicefabric-2.1.0 azure-mgmt-servicefabricmanagedclusters-2.0.0b6 azure-mgmt-servicelinker-1.2.0b2 azure-mgmt-signalr-2.0.0b1 azure-mgmt-sql-4.0.0b16 azure-mgmt-sqlvirtualmachine-1.0.0b5 azure-mgmt-storage-21.1.0 azure-mgmt-synapse-2.1.0b5 azure-mgmt-trafficmanager-1.0.0 azure-mgmt-web-7.2.0 azure-monitor-query-1.2.0 azure-multiapi-storage-1.2.0 azure-nspkg-3.0.2 azure-storage-blob-12.20.0 azure-storage-common-1.4.2 azure-synapse-accesscontrol-0.5.0 azure-synapse-artifacts-0.18.0 azure-synapse-managedprivateendpoints-0.4.0 azure-synapse-spark-0.2.0 bcrypt-4.1.3 boto3-1.34.131 botocore-1.34.131 cachetools-4.2.4 cohere-5.5.8 colorama-0.4.6 dataclasses-json-0.6.7 decorator-5.1.1 deeplake-3.9.12 deprecated-1.2.14 dill-0.3.8 dirtyjson-1.0.8 fabric-3.2.2 fastavro-1.9.5 google-api-core-2.10.2 google-api-python-client-2.31.0 google-auth-2.0.2 google-auth-oauthlib-0.4.6 google-cloud-storage-1.42.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 httpx-sse-0.4.0 humanfriendly-10.0 humbug-0.3.2 invoke-2.2.0 isodate-0.6.1 javaproperties-0.5.2 jedi-0.19.1 jmespath-1.0.1 jsondiff-2.0.0 knack-0.11.0 laspy-2.5.4 libdeeplake-0.0.134 llama-cloud-0.0.6 llama-index-agent-openai-0.2.7 llama-index-cli-0.1.12 llama-index-core-0.10.53.post1 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.2.3 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.25 llama-index-multi-modal-llms-openai-0.1.7 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.29 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.6 llama_index-0.10.53 lz4-4.3.3 marshmallow-3.21.3 msal-1.28.0 msal-extensions-1.2.0b1 msrest-0.7.1 msrestazure-0.6.4.post1 multiprocess-0.70.16 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 openai-1.35.10 parameterized-0.9.0 paramiko-3.4.0 pathos-0.3.2 pillow-10.2.0 pkginfo-1.11.1 portalocker-2.10.0 pox-0.3.4 ppft-1.7.6.8 pycomposefile-0.0.31 pydicom-2.4.4 pypdf-4.2.0 s3transfer-0.10.2 scp-0.13.6 semver-2.13.0 sentence-transformers-3.0.1 sshtunnel-0.1.5 striprtf-0.0.26 tiktoken-0.7.0 types-requests-2.32.0.20240622 typing-inspect-0.9.0 websocket-client-1.3.3 xmltodict-0.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "decorator",
                  "google"
                ]
              },
              "id": "0b27dfef571e43b8ab8377498c79e157"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W7_1dIq8MCdA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "9deec43c-008b-4379-a314-243ead2b279f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-6-caa8ea76ac41>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-caa8ea76ac41>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    |# Global\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "|# Global\n",
        "import os\n",
        "import getpass\n",
        "import textwrap\n",
        "import time\n",
        "import locale\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "from time import time\n",
        "import json\n",
        "\n",
        "# Llama Index\n",
        "from llama_index import SimpleDirectoryReader, Document, VectorStoreIndex, ServiceContext, StorageContext\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.node_parser import SimpleNodeParser, SentenceWindowNodeParser\n",
        "from llama_index.vector_stores import DeepLakeVectorStore\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.evaluation import generate_question_context_pairs, CorrectnessEvaluator, RelevancyEvaluator, FaithfulnessEvaluator, BatchEvalRunner, RetrieverEvaluator\n",
        "from llama_index.embeddings.cohereai import CohereEmbedding\n",
        "from llama_index.text_splitter import SentenceSplitter\n",
        "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
        "from llama_index.finetuning.embeddings.common import EmbeddingQAFinetuneDataset\n",
        "\n",
        "# OpenAI\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryTQSWpDDlvc"
      },
      "source": [
        "# <font color='289C4E'>2. Get API Keys ğŸ—ï¸<font><a class='anchor' id='keys'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eLEPf3HpRIn",
        "outputId": "ff6cf495-aeb8-400c-e461-aa64164faaa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available:\n",
        "  print('GPU available')\n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJmBj4QLDlvd",
        "outputId": "1421b59d-75f4-4227-9987-c84464ddffb6"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzbTluVBk4wo"
      },
      "source": [
        "# <font color='289C4E'>3. Data Ingestion and Preprocessing ğŸ“–<font><a class='anchor' id='data'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3mocchVDlvd"
      },
      "source": [
        "## <font color='289C4E'>3.1. Get Data ğŸ“”<font><a class='anchor' id='get'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-2-w1j7aIHI"
      },
      "source": [
        "The first step is to get our data. We will use the **SimpleDirectoryReader**, which will select the best file reader based on the file extensions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhiGDJJiDlvd"
      },
      "outputs": [],
      "source": [
        "# Define the folder path where data is stored\n",
        "data_folder = \"./data/\"\n",
        "\n",
        "# Create a SimpleDirectoryReader object to read data from the specified folder\n",
        "reader = SimpleDirectoryReader(\n",
        "    input_dir=data_folder,  # Set the input directory\n",
        "    recursive=True,  # Enable recursive scanning through subdirectories\n",
        ")\n",
        "\n",
        "# Create an empty list to store all documents\n",
        "documents = []\n",
        "\n",
        "# Iterate through the data returned by the reader and append each document to the 'documents' list\n",
        "for docs in reader.iter_data():\n",
        "    for doc in docs:\n",
        "        documents.append(doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR_v2L_gDlvd",
        "outputId": "2f45ffd4-bc12-485d-af97-540fbfb0d6d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> \n",
            "\n",
            "601 \n",
            "\n",
            "<class 'llama_index.schema.Document'>\n",
            "[Document(id_='7acc381b-c1e1-4bd6-b62a-48191b688c66', embedding=None, metadata={'page_label': '1', 'file_name': 'Harry Potter - Book 1 - The Sorcerers Stone.pdf', 'file_path': 'data\\\\Harry Potter - Book 1 - The Sorcerers Stone.pdf', 'file_type': 'application/pdf', 'file_size': 1049997, 'creation_date': '2024-01-05', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='fc9700d8723c56a9415dee8fbefef8c2946f925b547d3f428ac6c5d7fc0db462', text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f2e101b9-45bb-4bdc-8b03-20a41dfd07aa', embedding=None, metadata={'page_label': '2', 'file_name': 'Harry Potter - Book 1 - The Sorcerers Stone.pdf', 'file_path': 'data\\\\Harry Potter - Book 1 - The Sorcerers Stone.pdf', 'file_type': 'application/pdf', 'file_size': 1049997, 'creation_date': '2024-01-05', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='6a8489586deba2cd53631e5ca47b909808cd2c7a3379e7cdbe598e0f17e3486b', text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='00c0bef4-de8b-4864-9ee1-ded0bc6787d0', embedding=None, metadata={'page_label': '3', 'file_name': 'Harry Potter - Book 1 - The Sorcerers Stone.pdf', 'file_path': 'data\\\\Harry Potter - Book 1 - The Sorcerers Stone.pdf', 'file_type': 'application/pdf', 'file_size': 1049997, 'creation_date': '2024-01-05', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='4a55173158998b46268968ce6b77d5a5dfb04f7e540ca3f23912cecaca47cd75', text=\"HP\\t1\\t-\\tHarry\\tPotter\\tand\\tthe\\nSorcerer's\\tStone\\nHarry\\tPotter\\tand\\tthe\\tSorcerer's\\tStone\\n\\t\\n\\t\\nHarry\\tPotter\\n&\\nThe\\tSorcererâ€™s\\tStone\\n\\t\\n\\t\\nby\\t\\nJ.K.\\tRowling\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6acd8cae-2c69-490e-a9e1-1dc1872c0c65', embedding=None, metadata={'page_label': '4', 'file_name': 'Harry Potter - Book 1 - The Sorcerers Stone.pdf', 'file_path': 'data\\\\Harry Potter - Book 1 - The Sorcerers Stone.pdf', 'file_type': 'application/pdf', 'file_size': 1049997, 'creation_date': '2024-01-05', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='4f58d4537fff26b34244619b6da647f4a8916a92378b547a42561b7672f6f950', text=\"HP\\t1\\t-\\tHarry\\tPotter\\tand\\tthe\\nSorcerer's\\tStone\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
          ]
        }
      ],
      "source": [
        "print(type(documents), \"\\n\")\n",
        "print(len(documents), \"\\n\")\n",
        "print(type(documents[0]))\n",
        "print(documents[0:4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTGUugh4ZUsf"
      },
      "source": [
        "## <font color='289C4E'>3.2. Text Cleaning ğŸ§¹<font><a class='anchor' id='clean'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbGX4sw0auLp"
      },
      "source": [
        "The PDF has been converted to a list of 601 elements. We also see that the text of our data shows multiple consecutive spaces. This will require a cleaning step to preprocess the text.\n",
        "\n",
        "We will do it so, removing multiple consecutive spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7ZzoSLHZElD"
      },
      "outputs": [],
      "source": [
        "# Function to clean text by replacing multiple consecutive spaces with a single space\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# List comprehension to clean the text of each document in the 'documents' list\n",
        "cleaned_documents = [clean_text(doc.text) for doc in documents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SijAXq0aZMfH",
        "outputId": "e4b19ff8-a271-4608-d059-6908aad2fcd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> \n",
            "\n",
            "601 \n",
            "\n",
            "<class 'str'>\n",
            "['', '', \"HP 1 - Harry Potter and the Sorcerer's Stone Harry Potter and the Sorcerer's Stone Harry Potter & The Sorcererâ€™s Stone by J.K. Rowling \", \"HP 1 - Harry Potter and the Sorcerer's Stone\"]\n"
          ]
        }
      ],
      "source": [
        "print(type(cleaned_documents), \"\\n\")\n",
        "print(len(cleaned_documents), \"\\n\")\n",
        "print(type(cleaned_documents[0]))\n",
        "print(cleaned_documents[0:4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JRXQwYqiZ34"
      },
      "source": [
        "Now our text looks better, as the spaces has been removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxH59d1EDlve"
      },
      "source": [
        "## <font color='289C4E'>3.3. Merging Documents ğŸ¤¼â€â™‚ï¸ğŸ¤¼â€â™‚<font><a class='anchor' id='merge'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LTCHVNHiw9O"
      },
      "source": [
        "Let's put all documents together in a single one, creating a document object with the **Document** class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL77IvpRcq2f",
        "outputId": "e9256e2b-6587-483b-fb25-2403fd825972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'llama_index.schema.Document'>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "HP 1 - Harry Potter and the Sorcerer's Stone Harry Potter and the Sorcerer's Stone Harry Potter & The Sorcererâ€™s Stone by J.K. Rowling \n",
            "\n",
            "HP 1 - Harry Potter and the Sorcerer's Stone\n",
            "\n",
            "CHAPTER ONE THE BOY WHO LIVED M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people youâ€™d expect to be involved in anything strange or mysterious, because they just didnâ€™t hold with such nonsense. Mr. Dursley was the\n"
          ]
        }
      ],
      "source": [
        "# Merging Documents\n",
        "document = Document(text=\"\\n\\n\".join([doc for doc in cleaned_documents]))\n",
        "\n",
        "text_content = document.get_text()\n",
        "print(type(document))\n",
        "print(text_content[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwtIzK7sDlve"
      },
      "source": [
        "# <font color='289C4E'>4. Basic RAG Pipeline ğŸ¡<font><a class='anchor' id='basic'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNU9w3bwDlve"
      },
      "source": [
        "## <font color='289C4E'>4.1. Indexing Documents ğŸ“‡<font><a class='anchor' id='index'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D02PAykildzd"
      },
      "source": [
        "Indexing is a fundamental process for storing and organizing data from diverse sources into a vector store (**VectorStoreIndex**), a structure essential for efficient storage and retrieval (**ServiceContext** incorporates necessary configurations or services needed to generate vector representations). This process involves storing **text chunks** along with their corresponding **embedding** representations, capturing the **semantic meaning** of the text. These embeddings facilitate easy retrieval of chunks based on their semantic similarity. Embeddings are typically generated by specialized models like `BAAI/bge-small-en-v1.5` (Flag Embedding which is focused on RAG LLMs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WmN7ln8xDlve",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "0aa7eeaa-f14e-4b59-b3c4-2ad29aaa624e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2fc1d9cc193f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Imports required in this cell (otherwise it will show an error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mServiceContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Imports required in this cell (otherwise it will show an error)\n",
        "from llama_index import VectorStoreIndex\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "# Initialize an OpenAI language model (llm) with specific configurations\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1) # 0 to 2. When you set it higher, you'll get more random outputs.\n",
        "                                                     # When you set it lower, towards 0, the values are more deterministic.\n",
        "\n",
        "# Create a ServiceContext object with default configurations\n",
        "# This context incorporates required settings and services for generating vector representations\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\") # Flag Embedding: focus on RAG LLMs.\n",
        "\n",
        "\n",
        "# Create a VectorStoreIndex object by indexing the 'document' (text data) using the provided service context\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    [document],  # List containing the document(s) to be indexed\n",
        "    service_context=service_context)  # Incorporates the context for vector generation and indexing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIs2-0bEDlvf"
      },
      "source": [
        "## <font color='289C4E'>4.2. Basic Query Retrieval â”<font><a class='anchor' id='query'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0SGMFjDsc2F"
      },
      "source": [
        "Let's try to make a question and check the output text. We will compare later with other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Rw4J5pOZDlvf",
        "outputId": "b57b9f2e-79ef-435a-8f93-cf208f76714c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'index' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bf8fd622ccb8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initiate Query Engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mquery_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_query_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Submit a Query String\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m response = query_engine.query(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
          ]
        }
      ],
      "source": [
        "# Initiate Query Engine\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Submit a Query String\n",
        "response = query_engine.query(\n",
        "    \"Which are the main characters of the book?\")\n",
        "\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R-vWTz2b3Fx"
      },
      "source": [
        "# <font color='289C4E'>5. LLama Index: Deeplake RAG Pipeline ğŸŒŠ<font><a class='anchor' id='deeplake'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOyoeP4tcGzQ"
      },
      "source": [
        "## <font color='289C4E'>5.1. Nodes ğŸ” <font><a class='anchor' id='nodes'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTmJV6pPub7c"
      },
      "source": [
        "When passing documents to a vector store for indexing, there are two main alternatives:\n",
        "\n",
        "**1. Passing the Whole Document:**\n",
        "\n",
        "- Involves indexing the entire document as a single unit.\n",
        "- Suitable for smaller documents that fit comfortably within memory constraints.\n",
        "- Simpler indexing process but might lack granularity in capturing diverse content within larger documents.\n",
        "\n",
        "**2. Converting the Document into Nodes:**\n",
        "\n",
        "- Breaks down the document into smaller, manageable chunks or nodes.\n",
        "- Ideal for larger documents to prevent memory issues and for better granularity.\n",
        "- Enables indexing of specific sections or segments, improving the ability to capture diverse content within the document.\n",
        "\n",
        "As a general guideline, for larger documents, it's advantageous to break them down into smaller chunks or nodes before indexing. This approach not only helps in avoiding memory limitations but also allows for a more detailed and nuanced representation of the document's content. It facilitates better indexing granularity, potentially enhancing the retrieval and analysis of specific sections within the larger document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uihWhNz2AIP_"
      },
      "source": [
        "### <font color='289C4E'>5.1.1. Sentence Splitter ğŸª“<font><a class='anchor' id='split'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ruaJnd8cFYl"
      },
      "source": [
        "LlamaIndex features a **NodeParser** class designed to convert the content of documents into structured nodes automatically. The **SentenceSplitter** allows to break down your documents into sentences. This will require a pattern to decide where a sentence starts or stops (bullet points, points,..)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj6MmWbK9bT1"
      },
      "outputs": [],
      "source": [
        "# Create a SimpleNodeParser for parsing nodes from text content\n",
        "sentence_node_parser = SentenceSplitter.from_defaults(\n",
        "    paragraph_separator=r\"\\n(?:â—|-|\\s{2,}|\\.\\s|ï¼Ÿ|ï¼)\\n\",  # Regular expression pattern for paragraph separation\n",
        "    chunk_size=512,\n",
        "    include_prev_next_rel=True,   # Include previous and next relationships for nodes\n",
        "    include_metadata=True         # Include metadata for nodes (such as document information)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLiNWFAKzs-B",
        "outputId": "58ca595d-29af-418d-afca-24fdfe1d2d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Documents: 601\n",
            "Number of nodes: 766 with the current chunk size of 512\n"
          ]
        }
      ],
      "source": [
        "# Get nodes from the cleaned documents using the sentence_node_parser\n",
        "sentence_nodes = sentence_node_parser.get_nodes_from_documents([document])\n",
        "\n",
        "# Print information about the data and nodes\n",
        "print(f\"Number of Documents: {len(cleaned_documents)}\")  # Assuming 'cleaned_documents' contains preprocessed data\n",
        "print(f\"Number of nodes: {len(sentence_nodes)} with the current chunk size of {sentence_node_parser.chunk_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBA82YkOARDQ"
      },
      "source": [
        "### <font color='289C4E'>5.1.2. Sentence Window Node Parser ğŸªŸ<font><a class='anchor' id='window'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjQVmKrm8QhS"
      },
      "source": [
        "However, there is another Nodeparser which enhance the SentenceSplitter, the  **SentenceWindowNodeParser**. This splits a document into nodes, with each node being a sentence. Each node contains a window from the surrounding sentences in the metadata. Additionally, it contains a sentence splitter argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrf4VLUQ8P7h"
      },
      "outputs": [],
      "source": [
        "# Define a function that splits sentences based on the provided regular expression pattern\n",
        "def custom_sentence_splitter(text):\n",
        "    # Apply the regex pattern to split sentences\n",
        "    sentences = re.split(r\"\\n(?:â—|-|\\s{2,}|\\.\\s|ï¼Ÿ|ï¼)\\n\", text)\n",
        "    return sentences\n",
        "\n",
        "# Use the defined function as the sentence splitter\n",
        "window_parser = SentenceWindowNodeParser.from_defaults(\n",
        "    sentence_splitter=custom_sentence_splitter,  # Pass the callable function or list of functions\n",
        "    window_size=3,\n",
        "    include_prev_next_rel=True,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "window_nodes = window_parser.get_nodes_from_documents([document])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkkFFxEyBPXE"
      },
      "outputs": [],
      "source": [
        "# Print the nodes text\n",
        "print([x.text for x in window_nodes])\n",
        "print(window_nodes[1].metadata[\"original_text\"])\n",
        "print(window_nodes[1].metadata[\"window\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgbkiv-ykW0A"
      },
      "source": [
        "## <font color='289C4E'>5.2. Indexing ğŸ—‚ï¸<font><a class='anchor' id='index2'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INWifVeDVNzF"
      },
      "source": [
        "Let's now perform the indexing as before, but this time passing the nodes. As the OpenAI model has a maximum of 8192 tokens we will perform the Query in chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4YpHcZYH-EG"
      },
      "outputs": [],
      "source": [
        "# Define a function to chunk the original text\n",
        "def chunk_text(text, max_tokens_per_chunk):\n",
        "    chunked_text = []\n",
        "    current_chunk = []\n",
        "    current_chunk_token_count = 0\n",
        "\n",
        "    # Extract text from the document\n",
        "    tokens = text.split()  # Tokenize the original text\n",
        "\n",
        "    for token in tokens:\n",
        "        if current_chunk_token_count + len(token) < max_tokens_per_chunk:\n",
        "            current_chunk.append(token)\n",
        "            current_chunk_token_count += len(token)\n",
        "        else:\n",
        "            chunked_text.append(\" \".join(current_chunk))\n",
        "            current_chunk = [token]\n",
        "            current_chunk_token_count = len(token)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunked_text.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunked_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK7IMgEIfM_z"
      },
      "outputs": [],
      "source": [
        "# Imports required in this cell (otherwise it will show an error)\n",
        "from llama_index import VectorStoreIndex\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.schema import Node  # Import the Node class\n",
        "\n",
        "\n",
        "# Initialize an OpenAI language model (LLM) for question answering\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "\n",
        "# Define a local path for storing vectors using DeepLakeVectorStore\n",
        "dataset_path = \"/content/nodes/deep_lake_db\"\n",
        "\n",
        "# Create a DeepLakeVectorStore instance for vector storage with specified configurations\n",
        "vector_store = DeepLakeVectorStore(\n",
        "    dataset_path=dataset_path,  # Path to store vectors\n",
        "    overwrite=True,  # Overwrite if the dataset exists\n",
        "    exec_option=\"compute_engine\"  # Execution option (e.g., compute engine)\n",
        ")\n",
        "\n",
        "# Define an embedding model (text-embedding-ada-002) from OpenAI\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "# Create a ServiceContext incorporating the embedding model and the LLM\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
        "\n",
        "# Create a StorageContext incorporating the vector store for storage\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Define the maximum token count per chunk\n",
        "max_tokens_per_chunk = 8192  # Limit of the model\n",
        "\n",
        "# Chunk the nodes\n",
        "chunked_document = chunk_text(document.get_text(), max_tokens_per_chunk)\n",
        "\n",
        "# Initialize an empty list to store nodes\n",
        "window_nodes = []\n",
        "\n",
        "# Iterate through each chunk in chunked_document\n",
        "for index, chunk in enumerate(chunked_document):\n",
        "    # Create a Document object for each chunk of text\n",
        "    doc = Document(text=chunk, id_=str(index))  # Assign a unique ID to each document\n",
        "    # Create nodes from the Document object\n",
        "    nodes_from_chunk = window_parser.get_nodes_from_documents([doc])\n",
        "    # Extend the window_nodes list with nodes from the current chunk\n",
        "    window_nodes.extend(nodes_from_chunk)\n",
        "\n",
        "# Create a VectorStoreIndex for indexing chunked nodes with associated service and storage contexts\n",
        "vector_index = VectorStoreIndex(\n",
        "    window_nodes,\n",
        "    service_context=service_context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWdwCLeMAcTC"
      },
      "source": [
        "## <font color='289C4E'>5.3. Post Processing ğŸ“¥<font><a class='anchor' id='post'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5LGkxopAnPh"
      },
      "source": [
        "Then, during retrieval, before passing the retrieved sentences to the LLM, the single sentences are replaced with a window containing the surrounding sentences using the **MetadataReplacementNodePostProcessor**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um3oyS8AAmnH"
      },
      "outputs": [],
      "source": [
        "# Create a MetadataReplacementPostProcessor instance\n",
        "postproc = MetadataReplacementPostProcessor(\n",
        "    target_metadata_key=\"window\"  # Specifies the target metadata key for replacement\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5G6ff27D5BE"
      },
      "source": [
        "## <font color='289C4E'>5.4. Reranking ğŸ–ï¸<font><a class='anchor' id='rank'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF45vhoYNvil"
      },
      "source": [
        "The **SentenceTransformerRerank** uses the cross-encoders from the sentence-transformer package to re-order nodes, and returns the top N nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSjitDARCAhC",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create a SentenceTransformerRerank instance\n",
        "rerank = SentenceTransformerRerank(\n",
        "    top_n=3, model=\"BAAI/bge-reranker-base\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTIMg8HUkenL"
      },
      "source": [
        "## <font color='289C4E'>5.5. Query Engine ğŸš’<font><a class='anchor' id='engine'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3By-yS9bP54h"
      },
      "source": [
        "Now we are ready for our query. We will retrieve the top 10 most similar results and the score of the first 3 ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueOapfUcEKZQ",
        "outputId": "1730b774-7446-4504-bf03-b93b088146b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elapsed: 23.92s\n"
          ]
        }
      ],
      "source": [
        "# Create a query engine based on the vector index, with post-processing steps\n",
        "query_engine = vector_index.as_query_engine(\n",
        "    similarity_top_k=10,                   # Retrieve the top 10 most similar results\n",
        "    node_postprocessors=[postproc, rerank] # Apply post-processing techniques (postproc and rerank) to the retrieved nodes\n",
        ")\n",
        "\n",
        "# Record the current time\n",
        "now = time()\n",
        "\n",
        "# Execute a query using the prepared query engine\n",
        "response = query_engine.query(\n",
        "    \"Which are the main characters of the book?\",  # Query asking about the main characters of a book\n",
        ")\n",
        "\n",
        "# Calculate and print the elapsed time for the query execution\n",
        "print(f\"Elapsed: {round(time() - now, 2)}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lavP5BdAoiNC"
      },
      "source": [
        "Here we will check the the **score** of our query. The score represents the relevance or similarity measure between the query and the retrieved document. This score indicates the likelihood or degree to which the document is considered relevant to the query based on the model's understanding or learned representation of the text.\n",
        "\n",
        "For instance:\n",
        "\n",
        "- A higher score typically suggests greater relevance or similarity between the query and the document.\n",
        "\n",
        "- Lower scores imply lesser relevance or similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04wcoH5TMc8U",
        "outputId": "34784ce0-e006-4ff5-f701-f527c6013fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The main characters of the book are Harry Potter, Professor Dumbledore, Professor McGonagall, and Mr. and Mrs. Dursley.\n",
            "Top-K Results:\n",
            "1. Score: 0.7925972938537598 - Node ID: 7022f0e2-e41c-4ca7-8665-8e89ab80fc2f\n",
            "2. Score: 0.7906642556190491 - Node ID: e0c98d79-2533-4362-8ec7-5b55e5379281\n",
            "3. Score: 0.26864519715309143 - Node ID: 2f1da4a1-8fbd-4b22-8ec7-f319e890ed55\n",
            "Elapsed: 943.4s\n"
          ]
        }
      ],
      "source": [
        "# Process the response\n",
        "if response and response.source_nodes:\n",
        "    # Print response\n",
        "    print(response)\n",
        "    # Accessing and printing the top-k results from source_nodes\n",
        "\n",
        "    print(\"Top-K Results:\")\n",
        "    for rank, node_with_score in enumerate(response.source_nodes):\n",
        "        print(f\"{rank + 1}. Score: {node_with_score.score} - Node ID: {node_with_score.node.id_}\")\n",
        "        # Access other metadata or information from the node_with_score as needed\n",
        "else:\n",
        "    print(\"No results found.\")\n",
        "\n",
        "# Calculate and display the elapsed time\n",
        "print(f\"Elapsed: {round(time() - now, 2)}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmUDoAs9kh7C"
      },
      "source": [
        "# <font color='289C4E'>6. Model Evaluation ğŸš’<font><a class='anchor' id='model'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScFmZYl7VNzI"
      },
      "source": [
        "## <font color='289C4E'>6.1. Generate Questions â“<font><a class='anchor' id='questions'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRxRggMvRTHM"
      },
      "source": [
        "To evaluate the model we will use the **generate_question_context_pairs** function to generate an evaluation dataset of (question, context) pairs over the text corpus. This uses the LLM to auto-generate questions from each context chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3E_79EttyBR"
      },
      "outputs": [],
      "source": [
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Function to generate the evaluation dataset\n",
        "def generate_question(text):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a world class expert for generating questions based on provided context. \\\n",
        "                        You make sure the question can be answered by the text.\"},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": text,\n",
        "                },\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except:\n",
        "        question_string = \"No question generated\"\n",
        "        return question_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BJEMxKUtyEh",
        "outputId": "fd7c380e-4bf8-48e0-febd-33883517f5d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [07:02<00:00,  1.47s/it]\n"
          ]
        }
      ],
      "source": [
        "# Define the maximum token count per chunk\n",
        "max_tokens_per_chunk = 8192  # Limit of the model\n",
        "\n",
        "# Chunk the nodes\n",
        "chunked_nodes = chunk_nodes(window_nodes, max_tokens_per_chunk)\n",
        "\n",
        "# Create Node objects from the chunked text and add them to a new list\n",
        "chunked_node_objects = [Node(text=node_text) for node_text in chunked_nodes]\n",
        "\n",
        "qc_dataset = generate_question_context_pairs(\n",
        "    chunked_node_objects,\n",
        "    llm=llm,\n",
        "    num_questions_per_chunk=1\n",
        ")\n",
        "# We can save the dataset as a json file for later use.\n",
        "qc_dataset.save_json(\"qc_dataset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2grViENQwL6z"
      },
      "outputs": [],
      "source": [
        "# Load the questions dataset\n",
        "qc_dataset = EmbeddingQAFinetuneDataset.from_json(\n",
        "    \"qc_dataset.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkZUke_4VNzI"
      },
      "outputs": [],
      "source": [
        "# Load the JSON file\n",
        "with open('qc_dataset.json', 'r') as json_file:\n",
        "    qc_dataset = json.load(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAipI_x3VNzN"
      },
      "source": [
        "## <font color='289C4E'>6.2. Evaluation ğŸ’¯<font><a class='anchor' id='evaluation'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5EHycT0kCyV"
      },
      "source": [
        "Now we will evaluate in chunks and just some queries (due to the rate limits in OpenAI). The metrics that we will use are:\n",
        "\n",
        "- **Relevancy** evaluates whether the retrieved context and answer are relevant to the query.\n",
        "\n",
        "- **Faithfulness** evaluates the integrity of the answer, it faithfully represents the information in the retrieved context (the response from a query engine matches any source nodes) or, in other words, whether thereâ€™s a hallucination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDyRighTeAtn"
      },
      "outputs": [],
      "source": [
        "# Evaluation with top k 6 (it might exceed the tokens amount)\n",
        "\n",
        "i = 6\n",
        "\n",
        "# Set Faithfulness and Relevancy evaluators\n",
        "# query_engine = vector_index.as_query_engine(similarity_top_k=i)\n",
        "\n",
        "query_engine = vector_index.as_query_engine(\n",
        "    similarity_top_k=i,                   # Retrieve the top 10 most similar results\n",
        "    node_postprocessors=[postproc, rerank] # Apply post-processing techniques (postproc and rerank) to the retrieved nodes\n",
        ")\n",
        "# While we use GPT3.5-Turbo to answer questions\n",
        "llm2 = OpenAI(model=\"gpt-3.5-turbo-16k\", max_tokens=256)\n",
        "\n",
        "service_context_gpt = ServiceContext.from_defaults(llm=llm2)\n",
        "\n",
        "\n",
        "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_gpt)\n",
        "relevancy_evaluator = RelevancyEvaluator(service_context=service_context_gpt)\n",
        "\n",
        "# Run evaluation\n",
        "queries = list(qc_dataset.queries.values())\n",
        "batch_eval_queries = queries[:10]\n",
        "\n",
        "runner = BatchEvalRunner(\n",
        "{\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
        "workers=8,\n",
        ")\n",
        "eval_results = await runner.aevaluate_queries(\n",
        "    query_engine, queries=batch_eval_queries\n",
        ")\n",
        "faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
        "print(f\"top_{i} faithfulness_score: {faithfulness_score}\")\n",
        "\n",
        "relevancy_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['relevancy'])\n",
        "print(f\"top_{i} relevancy_score: {relevancy_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV6SZQ2CgIuw",
        "outputId": "fe578f33-1990-4ecf-d5db-46bb238d1d52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "top_2 faithfulness_score: 0.3333333333333333\n",
            "top_2 relevancy_score: 0.3333333333333333\n",
            "top_2 faithfulness_score: 1.0\n",
            "top_2 relevancy_score: 0.3333333333333333\n",
            "top_2 faithfulness_score: 0.3333333333333333\n",
            "top_2 relevancy_score: 0.3333333333333333\n",
            "top_2 faithfulness_score: 1.0\n",
            "top_2 relevancy_score: 1.0\n",
            "Average Faithfulness Score: 0.6666666666666666\n",
            "Average Relevancy Score: 0.5\n"
          ]
        }
      ],
      "source": [
        "# Evaluation with top k 2 (it might exceed the tokens amount)\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "i=2\n",
        "\n",
        "# Set Faithfulness and Relevancy evaluators\n",
        "# query_engine = vector_index.as_query_engine(similarity_top_k=i)\n",
        "\n",
        "query_engine = vector_index.as_query_engine(\n",
        "    similarity_top_k=i,                   # Retrieve the top 10 most similar results\n",
        "    node_postprocessors=[postproc, rerank] # Apply post-processing techniques (postproc and rerank) to the retrieved nodes\n",
        ")\n",
        "\n",
        "# While we use GPT3.5-Turbo to answer questions\n",
        "llm2 = OpenAI(model=\"gpt-3.5-turbo-16k\", max_tokens=256)\n",
        "\n",
        "service_context_gpt = ServiceContext.from_defaults(llm=llm2)\n",
        "\n",
        "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_gpt)\n",
        "relevancy_evaluator = RelevancyEvaluator(service_context=service_context_gpt)\n",
        "\n",
        "# Run evaluation\n",
        "queries = list(qc_dataset.queries.values())\n",
        "batch_eval_queries = queries[:10]\n",
        "\n",
        "# Split queries into chunks\n",
        "chunk_size = 3  # Define the size of each chunk\n",
        "chunks = [batch_eval_queries[i:i + chunk_size] for i in range(0, len(batch_eval_queries), chunk_size)]\n",
        "\n",
        "# Initialize variables to accumulate scores and counts\n",
        "total_faithfulness_score = 0\n",
        "total_relevancy_score = 0\n",
        "num_chunks = len(chunks)\n",
        "\n",
        "\n",
        "# Run evaluation for each chunk\n",
        "for chunk in chunks:\n",
        "    # Instantiate the runner for each chunk\n",
        "    runner = BatchEvalRunner(\n",
        "        {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
        "        workers=8,\n",
        "    )\n",
        "    eval_results = await runner.aevaluate_queries(query_engine, queries=chunk)\n",
        "\n",
        "    # Calculate metrics for the current chunk\n",
        "    faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
        "    relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
        "\n",
        "    # Accumulate scores for averaging\n",
        "    total_faithfulness_score += faithfulness_score\n",
        "    total_relevancy_score += relevancy_score\n",
        "\n",
        "    # Print the scores for the current chunk\n",
        "    print(f\"top_{i} faithfulness_score: {faithfulness_score}\")\n",
        "    print(f\"top_{i} relevancy_score: {relevancy_score}\")\n",
        "\n",
        "\n",
        "# Calculate averages\n",
        "average_faithfulness_score = total_faithfulness_score / num_chunks\n",
        "average_relevancy_score = total_relevancy_score / num_chunks\n",
        "\n",
        "# Print the averages\n",
        "print(f\"Average Faithfulness Score: {average_faithfulness_score}\")\n",
        "print(f\"Average Relevancy Score: {average_relevancy_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N7kYFYnxina",
        "outputId": "2f4ecf71-de20-4ce0-d967-0f5e1ede243f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "top_4 faithfulness_score: 1.0\n",
            "top_4 relevancy_score: 0.3333333333333333\n",
            "top_4 faithfulness_score: 1.0\n",
            "top_4 relevancy_score: 0.3333333333333333\n",
            "top_4 faithfulness_score: 1.0\n",
            "top_4 relevancy_score: 0.6666666666666666\n",
            "top_4 faithfulness_score: 1.0\n",
            "top_4 relevancy_score: 1.0\n",
            "Average Faithfulness Score: 1.0\n",
            "Average Relevancy Score: 0.5833333333333333\n"
          ]
        }
      ],
      "source": [
        "# Evaluation with top k 4 (it might exceed the tokens amount)\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "i=4\n",
        "\n",
        "# Set Faithfulness and Relevancy evaluators\n",
        "# query_engine = vector_index.as_query_engine(similarity_top_k=i)\n",
        "\n",
        "query_engine = vector_index.as_query_engine(\n",
        "    similarity_top_k=i,                   # Retrieve the top 10 most similar results\n",
        "    node_postprocessors=[postproc, rerank] # Apply post-processing techniques (postproc and rerank) to the retrieved nodes\n",
        ")\n",
        "# While we use GPT3.5-Turbo to answer questions\n",
        "llm2 = OpenAI(model=\"gpt-3.5-turbo-16k\", max_tokens=256)\n",
        "\n",
        "service_context_gpt = ServiceContext.from_defaults(llm=llm2)\n",
        "\n",
        "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_gpt)\n",
        "relevancy_evaluator = RelevancyEvaluator(service_context=service_context_gpt)\n",
        "\n",
        "# Run evaluation\n",
        "queries = list(qc_dataset.queries.values())\n",
        "batch_eval_queries = queries[:10]\n",
        "\n",
        "# Split queries into chunks\n",
        "chunk_size = 3  # Define the size of each chunk\n",
        "chunks = [batch_eval_queries[i:i + chunk_size] for i in range(0, len(batch_eval_queries), chunk_size)]\n",
        "\n",
        "# Initialize variables to accumulate scores and counts\n",
        "total_faithfulness_score = 0\n",
        "total_relevancy_score = 0\n",
        "num_chunks = len(chunks)\n",
        "\n",
        "\n",
        "# Run evaluation for each chunk\n",
        "for chunk in chunks:\n",
        "    # Instantiate the runner for each chunk\n",
        "    runner = BatchEvalRunner(\n",
        "        {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
        "        workers=8,\n",
        "    )\n",
        "    eval_results = await runner.aevaluate_queries(query_engine, queries=chunk)\n",
        "\n",
        "    # Calculate metrics for the current chunk\n",
        "    faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
        "    relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
        "\n",
        "    # Accumulate scores for averaging\n",
        "    total_faithfulness_score += faithfulness_score\n",
        "    total_relevancy_score += relevancy_score\n",
        "\n",
        "    # Print the scores for the current chunk\n",
        "    print(f\"top_{i} faithfulness_score: {faithfulness_score}\")\n",
        "    print(f\"top_{i} relevancy_score: {relevancy_score}\")\n",
        "\n",
        "# Calculate averages\n",
        "average_faithfulness_score = total_faithfulness_score / num_chunks\n",
        "average_relevancy_score = total_relevancy_score / num_chunks\n",
        "\n",
        "# Print the averages\n",
        "print(f\"Average Faithfulness Score: {average_faithfulness_score}\")\n",
        "print(f\"Average Relevancy Score: {average_relevancy_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JhDmb9qkjNC"
      },
      "source": [
        "We see in the results, that a higher similarity means, higher `faithfulness` and `context relevancy`. We can also appreciate a perfect score in the `faithfulness` with `top_k` . We assume that with a higher `top_k` the relevancy can be also close to 1.\n",
        "\n",
        "Due to the tokens limitation of the OpenAI model, it was not possible to run the code with more than top k 4. This might be different with an API KEY with a higher limit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vJmRJQZVNzO"
      },
      "source": [
        "## <font color='289C4E'>7. Further Steps ğŸ“ˆ<font><a class='anchor' id='steps'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03CvuvZQVNzO"
      },
      "source": [
        "This pipeline shows that a proper text preprocessing combined with a vector database, can end up in a outstanding model performance. However, there is always a path for improvement or different strategies for the data preprocessing. Some steps that can be carried out can be listed as follows:\n",
        "\n",
        "* Different database: `FAISS`, `Chroma`, `Pinecode`,...\n",
        "* Data cleaning: removal or numbers, punctuation,...\n",
        "* Different model: Huggingface `BAAI/bge-reranker-base` reranker model\n",
        "* Testset Generator: Ragas function `TestsetGenerator`that allow to generate questions and answers, which would allow to check metrics like MRR (Mean Reciprocal Rank) or Hit Rate by comparing with the groud truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDt-GDW4VNzO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "llama",
      "language": "python",
      "name": "llama"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}